# Pipeline de Procesamiento de Consumos

## ¬øQu√© hace este proyecto?
Basicamente, toma una cantidad de archivos de consumo y los procesa de manera eficiente. Lo mas importante es que lo hace en peque√±os lotes para no saturar la memoria.

### Caracteristicas principales:
- Procesa archivos en micro-lotes de hasta 1000 registros
- Detecta autom√°ticamente el delimitador de los archivos 
- Calcula tasas basadas en tarifas por destino
- Maneja valores m√≠nimos y maximos por a√±o
- Todo se guarda en PostgreSQL para consultas r√°pidas

## ¬øC√≥mo lo hice funcionar?
Despu√©s de varios intentos y mucho cafe ‚òï, logr√© implementar:
- Un sistema de logging que te dice que esta pasando
- Manejo de errores robusto (porque los datos nunca vienen perfectos)
- Procesamiento en lotes para no matar la memoria
- Una tabla de estadisticas que se actualiza en tiempo real

## Configuraci√≥n

### Opci√≥n 1: Instalaci√≥n Local
Necesitas:
- Python 3.10+
- PostgreSQL
- Un archivo `.env` con tus credenciales (te deje un ejemplo abajo)

```env
DB_USER=tu_usuario
DB_PASSWORD=tu_password
DB_HOST=localhost
DB_PORT=5432
DB_NAME=consumos_db
```
### Opci√≥n 2: Usando Docker üê≥
Docker:
1. Aseg√∫rate de tener Docker y Docker Compose instalados
2. Clona el repo
3. Coloca tus archivos de datos en la carpeta `datasets/`
4. Ejecuta:
```bash
docker-compose up --build
```
El pipeline se ejecutar√° autom√°ticamente con:
- PostgreSQL corriendo en un contenedor
- La aplicaci√≥n Python en otro contenedor
- Los datos persistidos en vol√∫menes Docker
- Todo configurado y listo para usar

## Estructura de archivos
Los archivos de entrada deben estar en una carpeta `datasets` con:
- `tarifa_por_destino.csv`: Tarifas por tipo de destino
- `minimos.csv`: Valores m√≠nimos por a√±o
- `maximos.csv`: Valores m√°ximos por a√±o
- Archivos `dataset-*.txt`: Tus datos de consumo

## ¬øC√≥mo lo uso?

### Si usas instalaci√≥n local:
1. Clona el repo
2. Instala las dependencias: `pip install -r requirements.txt`
3. Configura tu `.env`
4. Ejecuta: `python pipelinecarga.py`

### Si usas Docker:
1. Clona el repo
2. Coloca tus archivos en `datasets/`
3. Ejecuta `docker-compose up --build`
4. ¬°Disfruta! üéâ
